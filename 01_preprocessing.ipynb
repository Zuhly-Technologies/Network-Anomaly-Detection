{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "seconds = time.time()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process may take 5 to 10 minutes, depending on the performance of your computer.\n",
      "\n",
      "\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Monday-WorkingHours.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Tuesday-WorkingHours.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Wednesday-workingHours.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Friday-WorkingHours-Morning.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX  file is completed.\n",
      "\n",
      "String features before removing 'Label': ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']\n",
      "The pre-processing phase of the  Friday-WorkingHours-Afternoon-DDos.pcap_ISCX  file is completed.\n",
      "\n",
      "Total operation time: =  109.07501196861267 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##  CICIDS2017 csv files are required for the operation of the program.\n",
    "##  These files must be located under the \"CSVs\" folder in the same directory as the program.\n",
    "\n",
    "\n",
    "\n",
    "##  The purpose of this program is to clear the csv files containing CICIDS2017 data from errors.\n",
    "##  the faults observed are:\n",
    "##      1-   288602 of the entries in the file \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\" are empty / meaningless.\n",
    "##                   (e.g. \",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\")\n",
    "##\n",
    "##      2-  In the original csv files, while describing the Web Attack types such as Brute Force, XSS, Sql Injection, the character used is not recognized\n",
    "##                    by the Python-Pandas library and leads to the error.\n",
    "##                    this character (\"–\", Unicode code:8211) has been changed with another character (\"-\", Unicode code:45) to correct the error.\n",
    "##\n",
    "##   After the error correction, all the csv files were made into a single file (all_date.csv) to make it easier to process.\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    print(\"This process may take 5 to 10 minutes, depending on the performance of your computer.\\n\\n\\n\")\n",
    "    number=\"0123456789\"\n",
    "    # CSV files names:\n",
    "    csv_files=[\"Monday-WorkingHours.pcap_ISCX\",\n",
    "            \"Tuesday-WorkingHours.pcap_ISCX\",\n",
    "            \"Wednesday-workingHours.pcap_ISCX\",\n",
    "            \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX\",\n",
    "            \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX\",\n",
    "            \"Friday-WorkingHours-Morning.pcap_ISCX\",\n",
    "            \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX\",\n",
    "            \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX\",]\n",
    "\n",
    "    # Headers of column\n",
    "    main_labels=[\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Protocol\",\"Timestamp\",\"Flow Duration\",\"Total Fwd Packets\",\n",
    "    \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
    "    \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
    "    \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
    "    \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
    "    \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
    "    \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
    "    \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"faulty-Fwd Header Length\",\"Fwd Avg Bytes/Bulk\",\n",
    "    \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
    "    \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
    "    \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",\"Label\",\"External IP\"]\n",
    "\n",
    "    main_labels2=main_labels\n",
    "    main_labels=( \",\".join( i for i in main_labels ) )\n",
    "    main_labels=main_labels+\"\\n\"\n",
    "    flag=True\n",
    "    for i in range(len(csv_files)):\n",
    "        ths = open(str(i)+\".csv\", \"w\")\n",
    "        ths.write(main_labels)\n",
    "        with open(\"./CSVs/\"+csv_files[i]+\".csv\", \"r\") as file:\n",
    "            while True:\n",
    "                try:\n",
    "                    line=file.readline()\n",
    "                    if  line[0] in number:# this line eliminates the headers of CSV files and incomplete streams .\n",
    "                        if \" – \" in str(line): ##  if there is \"–\" character (\"–\", Unicode code:8211) in the flow ,  it will be chanced with \"-\" character ( Unicode code:45).\n",
    "                            line=(str(line).replace(\" – \",\" - \"))\n",
    "                        line=(str(line).replace(\"inf\",\"0\"))\n",
    "                        line=(str(line).replace(\"Infinity\",\"0\"))\n",
    "                        line=(str(line).replace(\"NaN\",\"0\"))\n",
    "                        \n",
    "                        ths.write(str(line))\n",
    "                    else:\n",
    "                        continue                       \n",
    "                except:\n",
    "                    break\n",
    "        ths.close()\n",
    "    \n",
    "    \n",
    "        df=pd.read_csv(str(i)+\".csv\",low_memory=False)\n",
    "        df = df.fillna(0).infer_objects()\n",
    "\n",
    "        if 'Label' in df.columns:\n",
    "            df['Label'] = df['Label'].astype(str)\n",
    "\n",
    "        string_features=[\"Flow Bytes/s\",\"Flow Packets/s\"]\n",
    "        for ii in string_features: #Some data in the \"Flow Bytes / s\" and \"Flow Packets / s\" columns are not numeric. Fixing this bug in this loop\n",
    "            df[ii]=df[ii].replace('Infinity', -1)\n",
    "            df[ii]=df[ii].replace('NaN', 0)\n",
    "            number_or_not=[]\n",
    "            for iii in df[ii]:\n",
    "                try:\n",
    "                    k=int(float(iii))\n",
    "                    number_or_not.append(int(k))\n",
    "                except:\n",
    "                    number_or_not.append(iii)\n",
    "            df[ii]=number_or_not\n",
    "\n",
    "\n",
    "\n",
    "        string_features=[]\n",
    "        for j in main_labels2: # In this section, non-numeric (string and / or categorical) properties (columns) are detected.\n",
    "            if df[j].dtype==\"object\":\n",
    "                string_features.append(j)\n",
    "\n",
    "        # print(string_features)\n",
    "        # try:\n",
    "        #     string_features.remove('Label')#The \"Label\" property was removed from the list. Because it has to remain \"categorical\" for using with different machine learning approach.\n",
    "        # except:\n",
    "        #     print(\"error!\")\n",
    "\n",
    "        # print(f\"Dtypes of columns in file {csv_files[i]}:\\n{df.dtypes}\\n\")\n",
    "\n",
    "        print(f\"String features before removing 'Label': {string_features}\")\n",
    "        if 'Label' in string_features:\n",
    "            string_features.remove('Label')\n",
    "        else:\n",
    "            print(\"'Label' column not found in string_features.\")\n",
    "\n",
    "        labelencoder_X = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "        for ii in string_features: ## In this loop, non-numeric (string and/or categorical) properties converted to numeric features.\n",
    "            try:\n",
    "                df[ii]=labelencoder_X.fit_transform(df[ii])\n",
    "            except:\n",
    "                df[ii]=df[ii].replace('Infinity', -1)\n",
    "        df=df.drop(main_labels2[61], axis=1) ## Column 61 is deleted because it is unnecessary, column 41 (\"Fwd Header Length\" feature) had be mistakenly rewritten.\n",
    "\n",
    "\n",
    "\n",
    "        ##All CSV files are merged into a single file.\n",
    "        if flag:\n",
    "            df.to_csv('all_data.csv' ,index = False)\n",
    "            df.to_csv('all_data_default.csv' ,index = False)\n",
    "            flag=False\n",
    "        else:\n",
    "            df.to_csv('all_data.csv' ,index = False,header=False,mode=\"a\")\n",
    "            df.to_csv('all_data_default.csv' ,index = False,header=False,mode=\"a\")\n",
    "        os.remove(str(i)+\".csv\")\n",
    "        print(\"The pre-processing phase of the \",csv_files[i],\" file is completed.\\n\")\n",
    "\n",
    "\n",
    "preprocess()\n",
    "\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (2672997, 85)\n",
      "Unique labels: ['BENIGN' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris' 'DoS Slowhttptest'\n",
      " 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed' 'Infiltration' 'Bot' 'PortScan'\n",
      " 'DDoS']\n",
      "Label\n",
      "BENIGN              2117531\n",
      "DoS Hulk             231073\n",
      "PortScan             158930\n",
      "DDoS                 128027\n",
      "DoS GoldenEye         10293\n",
      "FTP-Patator            7938\n",
      "SSH-Patator            5897\n",
      "DoS slowloris          5796\n",
      "DoS Slowhttptest       5499\n",
      "Bot                    1966\n",
      "Infiltration             36\n",
      "Heartbleed               11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "///////////////////////////////////////////////////////////////////////////////\n",
      "\n",
      "Reduced shape: (806025, 85)\n",
      "Unique labels after reduction: ['FTP-Patator' 'SSH-Patator' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk'\n",
      " 'DoS GoldenEye' 'Heartbleed' 'Infiltration' 'Bot' 'PortScan' 'DDoS'\n",
      " 'BENIGN']\n",
      "Label\n",
      "BENIGN              635259\n",
      "DDoS                128027\n",
      "DoS Hulk             23107\n",
      "PortScan             15893\n",
      "DoS GoldenEye         1029\n",
      "FTP-Patator            793\n",
      "SSH-Patator            589\n",
      "DoS slowloris          579\n",
      "DoS Slowhttptest       549\n",
      "Bot                    196\n",
      "Infiltration             3\n",
      "Heartbleed               1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def skewDataset(FTP_Patator, SSH_Patator, DoS_slowloris, DoS_Slowhttptest, DoS_Hulk, DoS_GoldenEye, Heartbleed, Infiltration, Bot, PortScan, DDoS, BENIGN_ratio):\n",
    "\n",
    "    df = pd.read_csv('all_data_default.csv')\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    print(\"Unique labels:\", df['Label'].unique())\n",
    "    print(df['Label'].value_counts())\n",
    "\n",
    "    params = {\n",
    "        'FTP-Patator': FTP_Patator,\n",
    "        'SSH-Patator': SSH_Patator,\n",
    "        'DoS slowloris': DoS_slowloris,\n",
    "        'DoS Slowhttptest': DoS_Slowhttptest,\n",
    "        'DoS Hulk': DoS_Hulk,\n",
    "        'DoS GoldenEye': DoS_GoldenEye,\n",
    "        'Heartbleed': Heartbleed,\n",
    "        'Infiltration': Infiltration,\n",
    "        'Bot': Bot,\n",
    "        'PortScan': PortScan,\n",
    "        'DDoS': DDoS\n",
    "    }\n",
    "\n",
    "    reduced_dfs = []\n",
    "\n",
    "    for label, keep in params.items():\n",
    "        if keep:\n",
    "\n",
    "            reduced_dfs.append(df[df['Label'] == label])\n",
    "        else:\n",
    "\n",
    "            label_df = df[df['Label'] == label]\n",
    "            keep_size = int(0.1 * len(label_df))\n",
    "            if keep_size > 0:\n",
    "                reduced_dfs.append(label_df.sample(keep_size, random_state=1))\n",
    "\n",
    "    if BENIGN_ratio < 1.0:\n",
    "        benign_df = df[df['Label'] == 'BENIGN']\n",
    "        keep_size = int(BENIGN_ratio * len(benign_df))\n",
    "        if keep_size > 0:\n",
    "            reduced_dfs.append(benign_df.sample(keep_size, random_state=1))\n",
    "    else:\n",
    "\n",
    "        reduced_dfs.append(df[df['Label'] == 'BENIGN'])\n",
    "\n",
    "    reduced_df = pd.concat(reduced_dfs).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n///////////////////////////////////////////////////////////////////////////////\\n\")\n",
    "    print(\"Reduced shape:\", reduced_df.shape)\n",
    "    print(\"Unique labels after reduction:\", reduced_df['Label'].unique())\n",
    "    print(reduced_df['Label'].value_counts())\n",
    "\n",
    "    return reduced_df\n",
    "\n",
    "reduced_df = skewDataset(FTP_Patator=False, \n",
    "                         SSH_Patator=False, \n",
    "                         DoS_slowloris=False, \n",
    "                         DoS_Slowhttptest=False, \n",
    "                         DoS_Hulk=False, \n",
    "                         DoS_GoldenEye=False, \n",
    "                         Heartbleed=False, \n",
    "                         Infiltration=False, \n",
    "                         Bot=False, \n",
    "                         PortScan=False, \n",
    "                         DDoS=True,\n",
    "                         BENIGN_ratio=0.3)\n",
    "\n",
    "reduced_df.to_csv('all_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
